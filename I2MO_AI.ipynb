{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BriceMichalski/BriceMichalski/blob/main/I2MO_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Env"
      ],
      "metadata": {
        "id": "mpuroIlr9xmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install -q datasets accelerate evaluate trl accelerate bitsandbytes peft"
      ],
      "metadata": {
        "collapsed": true,
        "id": "flLDzPqm9qnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNZIP DATASET\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = \"dataset.zip\"\n",
        "output_directory = \".\"\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall(output_directory)"
      ],
      "metadata": {
        "id": "C_wtFEDV6mY2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,TrainingArguments, Trainer\n",
        "from accelerate import Accelerator\n",
        "from peft import LoraConfig, get_peft_model,prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer,SFTConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "8tY9SDEH-PK8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SETTINGS\n",
        "DATASET_SIZE = 10\n",
        "BASE_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "OUTPUT_DIR = \"./workdir\"\n",
        "FINAL_MODEL_DIR = \"./build\"\n",
        "DATASET_PATH = \"./dataset\"\n",
        "HF_TOKEN = \"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\"\n",
        "FINAL_MODEL_ID = f\"{BASE_MODEL_ID.split('/')[1]}-i2mo-{DATASET_SIZE}\"\n",
        "\n",
        "accelerator = Accelerator()"
      ],
      "metadata": {
        "id": "3CRQgWzz6aWL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "lPJVlWTg88r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_articles_dataset(directory):\n",
        "    dataset = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".md\") and len(dataset) < DATASET_SIZE:\n",
        "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                dataset.append({\"text\": content})\n",
        "    return dataset\n",
        "\n",
        "articles = load_articles_dataset(DATASET_PATH)\n",
        "dataset = Dataset.from_list(articles)\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "c0uYVrxh9AWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Tokenization"
      ],
      "metadata": {
        "id": "Xi-VT961-HTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 2. Dataset Tokenization\n",
        "#\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, token=HF_TOKEN)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f\"Pad token : {tokenizer.pad_token}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    )\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "fKlfBoVQ-KrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "cjTMyf3l-o1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    token=HF_TOKEN,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "qYg54y8X-saf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up PEFT (Parameter-Efficient Fine-Tuning)"
      ],
      "metadata": {
        "id": "jqkXnkL1DX6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "g2rcs56zDfG-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare trainer"
      ],
      "metadata": {
        "id": "koqt-CEwAPoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{OUTPUT_DIR}/{FINAL_MODEL_ID}\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    per_device_train_batch_size=4,  # Plus grand batch size possible avec le GPU\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=500,\n",
        "    bf16=True,  # Utilisation du calcul en FP16 pour accélérer l'entraînement sur le GPU\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    args=training_args,\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "IGO4ytlXAVnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model"
      ],
      "metadata": {
        "id": "_T3ISTay_A8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "XUt0jAUe_EtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the adapter and model back together"
      ],
      "metadata": {
        "id": "hFEvQje6dsnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_model = trainer.model\n",
        "merged_model = adapter_model.merge_and_unload()\n",
        "\n",
        "trained_tokenizer = trainer.tokenizer"
      ],
      "metadata": {
        "id": "uR4D2Cf-dw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_tokenizer.save_pretrained(f\"{FINAL_MODEL_DIR}/{FINAL_MODEL_ID}\")\n",
        "merged_model.save_pretrained(f\"{FINAL_MODEL_DIR}/{FINAL_MODEL_ID}\")\n",
        "merged_model.config.save_pretrained(f\"{FINAL_MODEL_DIR}/{FINAL_MODEL_ID}\")"
      ],
      "metadata": {
        "id": "54GJ0gvXvQLc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip model"
      ],
      "metadata": {
        "id": "_I5iOYjSmo78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive(f\"{FINAL_MODEL_DIR}/{FINAL_MODEL_ID}\", 'zip', f\"{FINAL_MODEL_DIR}/{FINAL_MODEL_ID}\")"
      ],
      "metadata": {
        "id": "a4fXEtMMmq2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clear"
      ],
      "metadata": {
        "id": "M1tjpLz9Q5TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "O8YctsGCJvk0"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}